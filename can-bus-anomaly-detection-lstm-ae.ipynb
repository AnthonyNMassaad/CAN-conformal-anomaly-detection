{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T13:13:18.996815Z",
     "iopub.status.busy": "2025-09-16T13:13:18.996567Z",
     "iopub.status.idle": "2025-09-16T13:13:23.559638Z",
     "shell.execute_reply": "2025-09-16T13:13:23.558972Z",
     "shell.execute_reply.started": "2025-09-16T13:13:18.996797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SETUP & DEPENDENCIES\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from conformal import ConformalPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNTIME DEVICE\n",
    "# - GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS & GLOBAL HYPERPARAMETERS\n",
    "ROOT = Path.cwd()\n",
    "ARTIFACTS = ROOT / \"artifacts\"; ARTIFACTS.mkdir(parents=True, exist_ok=True)\n",
    "CKPTS = ROOT / \"checkpoints\"; CKPTS.mkdir(parents=True, exist_ok=True)\n",
    "BEST_MODEL_PATH = CKPTS / \"best_model.pth\"\n",
    "SCALER_PATH = ARTIFACTS / \"scaler.gz\"\n",
    "\n",
    "SELECT_SCORE   = \"MAE\"       # \"MSE\" or \"MAE\" or \"MAXAE\"\n",
    "TRAD_PCT       = 0.95        # τ_95 for traditional thresholding\n",
    "TRAD_SOURCE    = \"trainval\"  # \"trainval\" or \"calib\" for how to estimate tau\n",
    "ALPHA = 0.05    # target miscoverage (coverage = 1 - ALPHA)\n",
    "ALPHA_MINMAX  = (0.001, 0.20)   # bounds for adaptive α\n",
    "ADAPT_GAMMA   = 0.01  # learning rate for adaptive α\n",
    "N_ROWS_VIEW   = 5000  \n",
    "SEQUENCE_LENGTH = 20\n",
    "BATCH_SIZE = 128\n",
    "K_VALUES = [250, 10000, 1000000]\n",
    "SCORE_MODES = [\"MSE\", \"MAE\", \"MAXAE\"]\n",
    "\n",
    "TRAIN = False\n",
    "EPOCHS = 10\n",
    "PATIENCE = 3\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "    (\"dos\",   \"datasets/DoS_attack_dataset.csv\"),\n",
    "    (\"fuzzy\", \"datasets/Fuzzy_attack_dataset.csv\"),\n",
    "    (\"gear\",  \"datasets/gear_attack_dataset.csv\"),\n",
    "    (\"rpm\",   \"datasets/RPM_attack_dataset.csv\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:13:23.561236Z",
     "iopub.status.busy": "2025-09-16T13:13:23.560869Z",
     "iopub.status.idle": "2025-09-16T13:13:26.124295Z",
     "shell.execute_reply": "2025-09-16T13:13:26.123533Z",
     "shell.execute_reply.started": "2025-09-16T13:13:23.561218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LOAD BENIGN DATA\n",
    "normal_df = pd.read_csv('datasets/Attack_free_dataset.csv')\n",
    "print(\"Attack_free_dataset.csv loaded successfully.\")\n",
    "print(f\"Shape: {normal_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst 20 rows of the dataset:\")\n",
    "display(normal_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset Information:\")\n",
    "normal_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features / Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:13:26.125427Z",
     "iopub.status.busy": "2025-09-16T13:13:26.125174Z",
     "iopub.status.idle": "2025-09-16T13:13:26.381127Z",
     "shell.execute_reply": "2025-09-16T13:13:26.380146Z",
     "shell.execute_reply.started": "2025-09-16T13:13:26.125399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING & SPLITTING\n",
    "# - time_delta = diff of timestamp\n",
    "# - Split: 80/10/10 into Train / Val / Calib on benign only\n",
    "normal_df['time_delta'] = normal_df['timestamp'].diff().fillna(0)\n",
    "\n",
    "features = ['time_delta', 'can_id', 'dlc'] + [f'data_{i}' for i in range(8)]\n",
    "data = normal_df[features].values\n",
    "\n",
    "n = len(data)\n",
    "train_end = int(0.80 * n)\n",
    "val_end   = int(0.90 * n)\n",
    "\n",
    "X_train_raw = data[:train_end]\n",
    "X_val_raw   = data[train_end:val_end]\n",
    "X_calib_raw = data[val_end:]\n",
    "\n",
    "print(f\"Training={len(X_train_raw)}\\nValidation={len(X_val_raw)}\\nCalibration={len(X_calib_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE TRAIN VIEW\n",
    "print(\"\\nFirst 20 rows of the training set:\")\n",
    "display(pd.DataFrame(X_train_raw, columns=features).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:13:26.382963Z",
     "iopub.status.busy": "2025-09-16T13:13:26.382693Z",
     "iopub.status.idle": "2025-09-16T13:13:26.918249Z",
     "shell.execute_reply": "2025-09-16T13:13:26.917530Z",
     "shell.execute_reply.started": "2025-09-16T13:13:26.382941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SCALING & PERSISTENCE\n",
    "# - Fit MinMax on Train only, then transform (Train/Val/Calib)\n",
    "# - Persist scaler to reproduce inference-time transforms\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_raw)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train_raw)\n",
    "X_val_scaled = scaler.transform(X_val_raw)\n",
    "X_calib_scaled = scaler.transform(X_calib_raw)\n",
    "\n",
    "joblib.dump(scaler, SCALER_PATH)\n",
    "print(f\"Scaler fitted on training data and saved to {SCALER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECK POST-SCALING\n",
    "print(\"\\nFirst 20 rows of the training set:\")\n",
    "display(pd.DataFrame(X_train_scaled, columns=features).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:13:26.919126Z",
     "iopub.status.busy": "2025-09-16T13:13:26.918906Z",
     "iopub.status.idle": "2025-09-16T13:13:30.431885Z",
     "shell.execute_reply": "2025-09-16T13:13:30.431131Z",
     "shell.execute_reply.started": "2025-09-16T13:13:26.919109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# SEQUENCE BUILDER & DATALOADERS\n",
    "# - Convert row-wise features to (seq_len, feat_dim) windows\n",
    "# - Create PyTorch datasets/loaders for AE (input == target)\n",
    "def create_sequences(data, sequence_length):\n",
    "    if len(data) < sequence_length:\n",
    "        return np.empty((0, sequence_length, data.shape[1]), dtype=np.float32)\n",
    "    out = np.empty((len(data) - sequence_length + 1, sequence_length, data.shape[1]), dtype=np.float32)\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = data[i:i+sequence_length]\n",
    "    return out\n",
    "\n",
    "train_sequences = create_sequences(X_train_scaled, SEQUENCE_LENGTH)\n",
    "val_sequences = create_sequences(X_val_scaled, SEQUENCE_LENGTH)\n",
    "calib_sequences = create_sequences(X_calib_scaled, SEQUENCE_LENGTH)\n",
    "\n",
    "print(f\"Shape of training sequences: {train_sequences.shape}\\nShape of validation sequences: {val_sequences.shape}\\nShape of calibration sequences: {calib_sequences.shape}\")\n",
    "\n",
    "train_sequences_tensor = torch.tensor(train_sequences, dtype=torch.float32)\n",
    "val_sequences_tensor = torch.tensor(val_sequences, dtype=torch.float32)\n",
    "calib_sequences_tensor = torch.tensor(calib_sequences, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_sequences_tensor, train_sequences_tensor)\n",
    "val_dataset = TensorDataset(val_sequences_tensor, val_sequences_tensor)\n",
    "calib_dataset = TensorDataset(calib_sequences_tensor, calib_sequences_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calib_loader = DataLoader(calib_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\\nNumber of validation batches: {len(val_loader)}\\nNumber of calibration batches: {len(calib_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T13:13:53.917879Z",
     "iopub.status.busy": "2025-09-16T13:13:53.917155Z",
     "iopub.status.idle": "2025-09-16T13:13:53.929899Z",
     "shell.execute_reply": "2025-09-16T13:13:53.929202Z",
     "shell.execute_reply.started": "2025-09-16T13:13:53.917855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LSTM AUTOENCODER\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, _ = self.lstm(x, (hidden, cell)) \n",
    "        reconstruction = self.fc(output)\n",
    "        return reconstruction\n",
    "\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, seq_len, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "        self.decoder = Decoder(input_dim, hidden_dim, num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden, cell = self.encoder(x)\n",
    "        decoder_input = torch.zeros_like(x).to(device)\n",
    "        \n",
    "        reconstruction = self.decoder(decoder_input, hidden, cell)\n",
    "        return reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL INSTANTIATION & OPTIMIZER\n",
    "# - Default MSE loss for AE\n",
    "# - Optimizer: Adam\n",
    "INPUT_DIM = train_sequences.shape[2] \n",
    "HIDDEN_DIM = 64\n",
    "\n",
    "model = LSTMAutoencoder(INPUT_DIM, HIDDEN_DIM, SEQUENCE_LENGTH).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T18:00:42.065442Z",
     "iopub.status.busy": "2025-09-15T18:00:42.064733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "def train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, patience=PATIENCE):\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best, bad = np.inf, 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        bar = tqdm(train_loader, desc=f\"Epoch {ep+1}/{epochs}\", leave=False)\n",
    "        for xb, yb in bar:\n",
    "            xb, yb = xb.float().to(device), yb.float().to(device)\n",
    "            opt.zero_grad()\n",
    "            yhat = model(xb)\n",
    "            loss = criterion(yhat, yb)\n",
    "            loss.backward(); opt.step()\n",
    "            tr_loss += loss.item()\n",
    "            bar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "        tr_loss /= max(1, len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.float().to(device), yb.float().to(device)\n",
    "                va_loss += criterion(model(xb), yb).item()\n",
    "        va_loss /= max(1, len(val_loader))\n",
    "        print(f\"Epoch {ep+1}: train {tr_loss:.6f} | val {va_loss:.6f}\")\n",
    "\n",
    "        if va_loss < best:\n",
    "            best = va_loss; bad = 0\n",
    "            torch.save({\"model_state_dict\": model.state_dict()}, BEST_MODEL_PATH)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "if TRAIN:\n",
    "    train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD BEST CHECKPOINT FOR INFERENCE\n",
    "checkpoint = torch.load(BEST_MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORING UTILITIES\n",
    "# - score_batch: compute nonconformity per sequence (MSE/MAE/MAXAE)\n",
    "# - predict_scores\n",
    "def score_batch(pred, tgt, mode: str):\n",
    "    \"\"\"Return (B,) score per sequence according to nonconformity mode.\"\"\"\n",
    "    diff = (pred - tgt).abs()\n",
    "    if mode == \"MSE\":\n",
    "        return (diff**2).mean(dim=(1, 2)).cpu().numpy()\n",
    "    if mode == \"MAE\":\n",
    "        return diff.mean(dim=(1, 2)).cpu().numpy()\n",
    "    if mode == \"MAXAE\":\n",
    "        return diff.max(dim=2).values.max(dim=1).values.cpu().numpy()\n",
    "    raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "def predict_scores(model, loader, mode: str):\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.float().to(device), yb.float().to(device)\n",
    "            yhat = model(xb)\n",
    "            out.append(score_batch(yhat, yb, mode))\n",
    "    return np.concatenate(out) if out else np.array([])\n",
    "\n",
    "def benign_scores_for_threshold(mode: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return benign scores used to compute the traditional threshold τ.\n",
    "    Default: Train+Val (no calibration leakage).\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    if TRAD_SOURCE.lower() in {\"trainval\", \"train\", \"val\"}:\n",
    "        if TRAD_SOURCE.lower() in {\"trainval\", \"train\"}:\n",
    "            parts.append(predict_scores(model, train_loader, mode))\n",
    "        if TRAD_SOURCE.lower() in {\"trainval\", \"val\"}:\n",
    "            parts.append(predict_scores(model, val_loader, mode))\n",
    "    elif TRAD_SOURCE.lower() == \"calib\":\n",
    "        parts.append(predict_scores(model, calib_loader, mode))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown TRAD_SOURCE={TRAD_SOURCE}\")\n",
    "    return np.concatenate(parts) if parts else np.array([])\n",
    "\n",
    "def compute_tau(scores: np.ndarray, pct: float) -> float:\n",
    "    if scores.size == 0:\n",
    "        raise ValueError(\"[traditional] No benign scores to compute τ.\")\n",
    "    return float(np.quantile(scores, pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCORE FROM CSV (ATTACK FILES)\n",
    "# - Build time_delta, scale, window into sequences, infer scores\n",
    "def scores_from_csv(csv_path, features, scaler, L, batch_size=BATCH_SIZE, mode=\"MAE\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"time_delta\"] = df[\"timestamp\"].diff().fillna(0)\n",
    "    X = scaler.transform(df[features].values)\n",
    "    if len(X) < L:\n",
    "        return np.array([]), np.array([]) \n",
    "\n",
    "    seqs = create_sequences(X, L)\n",
    "    dl = DataLoader(TensorDataset(torch.tensor(seqs, dtype=torch.float32),\n",
    "                                  torch.tensor(seqs, dtype=torch.float32)),\n",
    "                    batch_size=batch_size, shuffle=False)\n",
    "    scores = predict_scores(model, dl, mode)\n",
    "\n",
    "    seq_labels = np.array([])\n",
    "    if \"label\" in df.columns and len(df) >= L:\n",
    "        rows = (\n",
    "            (df[\"label\"].astype(str).str.upper().isin([\"1\",\"T\",\"TRUE\"])) |\n",
    "            (df[\"label\"].astype(str).isin([\"1\"]))\n",
    "        ).astype(int).to_numpy()\n",
    "        seq_labels = np.array([1 if np.any(rows[i:i+L]) else 0 for i in range(len(rows)-L+1)], dtype=int)\n",
    "    return scores, seq_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING: COVERAGE ONLY (FIXED THRESHOLD)\n",
    "def plot_coverage_only_traditional(\n",
    "    scores: np.ndarray,\n",
    "    tau: float,\n",
    "    L: int,\n",
    "    atk_row_start: int | None,\n",
    "    save_path,\n",
    "    attack_name: str = \"DoS\",\n",
    "    n_rows_view: int = 5000,\n",
    "):\n",
    "    if scores.size == 0:\n",
    "        print(\"[plot_coverage_only_traditional] Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    x_end = np.arange(scores.size) + (L - 1)\n",
    "    mask = x_end < n_rows_view\n",
    "    if not np.any(mask):\n",
    "        print(f\"[plot_coverage_only_traditional] No sequences within first {n_rows_view} rows.\")\n",
    "        return\n",
    "\n",
    "    x = x_end[mask]\n",
    "    s = scores[mask]\n",
    "    covered = (s <= tau).astype(int)\n",
    "    coverage = np.cumsum(covered) / np.arange(1, covered.size + 1)\n",
    "\n",
    "    plt.figure(figsize=(11, 4))\n",
    "    plt.plot(x, coverage, linewidth=2, label=\"coverage (≤ τ)\")\n",
    "    if atk_row_start is not None and atk_row_start <= x[-1]:\n",
    "        plt.axvline(atk_row_start, linestyle=\":\", linewidth=1.25, label=\"attack start\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel(\"row index\")\n",
    "    plt.ylabel(\"coverage (≤ τ)\")\n",
    "    plt.title(f\"{attack_name.upper()} – coverage only (traditional threshold)\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING: HISTOGRAM / CONFUSION MATRIX\n",
    "def plot_hist(arr, title, xlabel, save_path=None, vline=None, vlabel=None):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.histplot(arr, bins=50, kde=True)\n",
    "    if vline is not None:\n",
    "        plt.axvline(vline, linestyle=\"--\", linewidth=2, label=vlabel)\n",
    "        plt.legend(fontsize=12)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(\"Count\"); plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_cm(y_true, y_pred, title, save_path=None):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    plt.figure(figsize=(4.2,3.6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cbar=False,\n",
    "                xticklabels=[\"Normal\",\"Attack\"], yticklabels=[\"Normal\",\"Attack\"])\n",
    "    plt.title(title); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "# TRADITIONAL TIMELINE PLOT (NO CONFORMAL)\n",
    "def plot_rows_timeline_traditional(\n",
    "    scores: np.ndarray,\n",
    "    L: int,\n",
    "    tau: float,\n",
    "    atk_row_start: int | None,\n",
    "    save_path,\n",
    "    mode_label: str = \"MSE\",\n",
    "    attack_name: str = \"DoS\",\n",
    "    smooth: int = 25,\n",
    "    n_rows_view: int = 5000,\n",
    "    y_clip: float = 0.995,\n",
    "    y_pad: float = 1.05,\n",
    "):\n",
    "    COL_SCORE, COL_TAU, COL_ATTACK, COL_COVER = \"#1f77b4\", \"#2ca02c\", \"#ff7f0e\", \"#d62728\"\n",
    "\n",
    "    if scores.size == 0:\n",
    "        print(\"[plot_rows_timeline_traditional] Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    x_end = np.arange(scores.size) + (L - 1)\n",
    "    mask = x_end < n_rows_view\n",
    "    if not np.any(mask):\n",
    "        print(f\"[plot_rows_timeline_traditional] No sequences within first {n_rows_view} rows.\")\n",
    "        return\n",
    "\n",
    "    x = x_end[mask]\n",
    "    s = scores[mask]\n",
    "    covered = (s <= tau).astype(int)\n",
    "    coverage = np.cumsum(covered) / np.arange(1, covered.size + 1)\n",
    "\n",
    "    s_smooth = pd.Series(s).rolling(max(5, smooth), min_periods=1).mean().to_numpy()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(11, 4))\n",
    "    ax1.plot(x, s_smooth, color=COL_SCORE, linewidth=2, label=f\"{mode_label} (reconstruction)\")\n",
    "    ax1.axhline(tau, color=COL_TAU, linestyle=\"--\", linewidth=1.8, label=r\"threshold $\\tau_{95}$\")\n",
    "\n",
    "    if atk_row_start is not None and atk_row_start <= x[-1]:\n",
    "        ax1.axvline(atk_row_start, color=COL_ATTACK, linestyle=\":\", linewidth=1.4, label=\"attack start\")\n",
    "\n",
    "    ax1.set_xlabel(\"CAN frame index\", fontsize=14)\n",
    "    ax1.set_ylabel(f\"{mode_label} score\", color=COL_SCORE, labelpad=10, fontsize=14)\n",
    "    ax1.tick_params(axis=\"y\", colors=COL_SCORE)\n",
    "\n",
    "    ymax = float(np.quantile(s, y_clip)) * y_pad\n",
    "    if not np.isfinite(ymax) or ymax <= 0:\n",
    "        ymax = max(1.0, float(s.max()) * y_pad)\n",
    "    ax1.set_ylim(0.0, ymax)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, coverage, color=COL_COVER, linewidth=2, label=\"coverage\")\n",
    "    ax2.set_ylabel(\"coverage\", color=COL_COVER, labelpad=10, fontsize=14)\n",
    "    ax2.tick_params(axis=\"y\", colors=COL_COVER, labelsize=12)\n",
    "    ax2.tick_params(axis=\"y\", colors=COL_COVER)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    \n",
    "    ax1.legend(\n",
    "        lines + lines2,\n",
    "        labels + labels2,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.15),\n",
    "        ncol=2,\n",
    "        frameon=False,\n",
    "        handlelength=2.5,\n",
    "        columnspacing=1.2,\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    fig.subplots_adjust(top=0.98, right=0.86, bottom=0.01)\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# FIXED TIMELINE PLOT (WITH CONFORMAL)\n",
    "def plot_rows_timeline_fixed(\n",
    "    scores: np.ndarray,\n",
    "    seq_labels: np.ndarray | None,\n",
    "    q_fixed: float,\n",
    "    alpha: float,\n",
    "    L: int,\n",
    "    atk_row_start: int | None,\n",
    "    save_path,\n",
    "    mode_label: str = \"MSE\",\n",
    "    attack_name: str = \"DoS\",\n",
    "    smooth: int = 25,\n",
    "    n_rows_view: int = 5000, \n",
    "    y_clip: float = 0.995,\n",
    "    y_pad: float = 1.05,\n",
    "):\n",
    "    COL_MSE, COL_Q, COL_ATTACK, COL_COVER, COL_TARGET = \"#1f77b4\", \"#2ca02c\", \"#ff7f0e\", \"#d62728\", \"#7f7f7f\"\n",
    "\n",
    "    Nseq = scores.size\n",
    "    if Nseq == 0:\n",
    "        print(\"[plot_rows_timeline_fixed] Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    x_end = np.arange(Nseq) + (L - 1)\n",
    "    mask = x_end < n_rows_view\n",
    "    if not np.any(mask):\n",
    "        print(f\"[plot_rows_timeline_fixed] No sequences end before row {n_rows_view}.\")\n",
    "        return\n",
    "\n",
    "    x = x_end[mask]\n",
    "    s = scores[mask]\n",
    "\n",
    "    covered = (s <= q_fixed).astype(int)\n",
    "    coverage = np.cumsum(covered) / np.arange(1, covered.size + 1)\n",
    "\n",
    "    s_smooth = pd.Series(s).rolling(max(5, smooth), min_periods=1).mean().to_numpy()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(11, 4))\n",
    "    ax1.plot(x, s_smooth, color=COL_MSE, linewidth=2, label=f\"{mode_label} (reconstruction)\")\n",
    "    ax1.axhline(q_fixed, color=COL_Q, linestyle=\"--\", linewidth=1.8, label=fr\"threshold $\\hat{{q}}$ (α={alpha})\")\n",
    "\n",
    "    if atk_row_start is not None:\n",
    "        ax1.axvline(atk_row_start, color=COL_ATTACK, linestyle=\":\", linewidth=1.4, label=\"attack start\")\n",
    "\n",
    "    ax1.set_xlabel(\"CAN frame index\")\n",
    "    ax1.set_ylabel(f\"{mode_label} score\", color=COL_MSE)\n",
    "    ax1.tick_params(axis=\"y\", colors=COL_MSE)\n",
    "\n",
    "    # y-scale to see fluctuations\n",
    "    ymax = float(np.quantile(s, y_clip)) * y_pad\n",
    "    if not np.isfinite(ymax) or ymax <= 0:\n",
    "        ymax = max(1.0, float(s.max()) * y_pad)\n",
    "    ax1.set_ylim(0.0, ymax)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, coverage, color=COL_COVER, linewidth=2, label=\"coverage\")\n",
    "    ax2.axhline(1 - alpha, color=COL_TARGET, linestyle=\"--\", linewidth=1.5, label=f\"target 1-α = {1 - alpha:.2f}\")\n",
    "    ax2.set_ylabel(\"coverage\", color=COL_COVER)\n",
    "    ax2.tick_params(axis=\"y\", colors=COL_COVER)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# ADAPTIVE TIMELINE PLOT (WITH CONFORMAL)\n",
    "def plot_rows_timeline_adaptive(\n",
    "    scores: np.ndarray,\n",
    "    seq_labels: np.ndarray | None,\n",
    "    alpha_series: np.ndarray,\n",
    "    q_series: np.ndarray,\n",
    "    alpha0: float,\n",
    "    L: int,\n",
    "    atk_row_start: int | None,\n",
    "    save_path,\n",
    "    mode_label: str = \"MSE\",\n",
    "    attack_name: str = \"DoS\",\n",
    "    smooth: int = 25,\n",
    "    n_rows_view: int = 5000,\n",
    "    y_clip: float = 0.995,\n",
    "    y_pad: float = 1.05,\n",
    "):\n",
    "    COL_MSE, COL_Q, COL_ATTACK, COL_COVER, COL_TARGET, COL_ALPHA = \\\n",
    "        \"#1f77b4\", \"#2ca02c\", \"#ff7f0e\", \"#d62728\", \"#7f7f7f\", \"#9467bd\"\n",
    "\n",
    "    if scores.size == 0:\n",
    "        print(\"[plot_rows_timeline_adaptive] Nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    x_end = np.arange(scores.size) + (L - 1)\n",
    "    mask = x_end < n_rows_view\n",
    "    if not np.any(mask):\n",
    "        print(f\"[plot_rows_timeline_adaptive] No sequences within first {n_rows_view} rows.\")\n",
    "        return\n",
    "\n",
    "    x = x_end[mask]\n",
    "    s = scores[mask]\n",
    "    q = q_series[mask]\n",
    "    a = alpha_series[mask]\n",
    "\n",
    "    covered = (s <= q).astype(int)\n",
    "    coverage = np.cumsum(covered) / np.arange(1, covered.size + 1)\n",
    "\n",
    "    s_smooth = pd.Series(s).rolling(max(5, smooth), min_periods=1).mean().to_numpy()\n",
    "    q_smooth = pd.Series(q).rolling(max(5, smooth), min_periods=1).mean().to_numpy()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(11, 4))\n",
    "    ax1.plot(x, s_smooth, color=COL_MSE, linewidth=2, label=f\"{mode_label} (reconstruction)\")\n",
    "    ax1.plot(x, q_smooth, color=COL_Q, linestyle=\"--\", linewidth=1.8, label=r\"adaptive $q_t$\")\n",
    "\n",
    "    if atk_row_start is not None and atk_row_start <= x[-1]:\n",
    "        ax1.axvline(atk_row_start, color=COL_ATTACK, linestyle=\":\", linewidth=1.4, label=\"attack start\")\n",
    "\n",
    "    ax1.set_xlabel(\"CAN frame index\", fontsize=14)\n",
    "    ax1.set_ylabel(f\"{mode_label} score\", color=COL_MSE, fontsize=14)\n",
    "    ax1.tick_params(axis=\"y\", colors=COL_MSE)\n",
    "\n",
    "    ymax = float(np.quantile(s, y_clip)) * y_pad\n",
    "    if not np.isfinite(ymax) or ymax <= 0:\n",
    "        ymax = max(1.0, float(s.max()) * y_pad)\n",
    "    ax1.set_ylim(0.0, ymax)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, coverage, color=COL_COVER, linewidth=2, label=\"coverage\")\n",
    "    ax2.axhline(1 - alpha0, color=COL_TARGET, linestyle=\"--\", linewidth=1.5, label=f\"target 1-α = {1 - alpha0:.2f}\")\n",
    "    ax2.set_ylabel(\"coverage\", color=COL_COVER, labelpad=10, fontsize=14)\n",
    "    ax2.tick_params(axis=\"y\", colors=COL_COVER, labelsize=12)\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines.right.set_position((\"axes\", 1.10))\n",
    "    ax3.plot(x, a, color=COL_ALPHA, linewidth=1.2, alpha=0.8, label=r\"$\\alpha_t$\")\n",
    "    ax3.set_ylabel(r\"$\\alpha_t$\", color=COL_ALPHA, labelpad=12, fontsize=20)\n",
    "    ax3.tick_params(axis=\"y\", colors=COL_ALPHA, labelsize=12)\n",
    "    ax3.set_ylim(0, max(0.5, a.max() * 1.1))\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    lines3, labels3 = ax3.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2 + lines3, labels + labels2 + labels3, loc=\"best\")\n",
    "\n",
    "    ax1.legend(\n",
    "        lines + lines2 + lines3,\n",
    "        labels + labels2 + labels3,\n",
    "        loc=\"upper center\",\n",
    "        bbox_to_anchor=(0.5, 1.15),  \n",
    "        ncol=3,                       \n",
    "        frameon=False,\n",
    "        handlelength=2.5,\n",
    "        columnspacing=1.2,\n",
    "        borderaxespad=0.0,\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "    fig.subplots_adjust(top=1.2, right=0.86, bottom=0)\n",
    "\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ATTACK SCORES and LABELS\n",
    "# - Build sequences from attack CSV, infer scores and seq-level labels\n",
    "# - Detect first attack row to align timeline plots\n",
    "def load_attack_scores(csv_path, features, scaler, L, mode):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"time_delta\"] = df[\"timestamp\"].diff().fillna(0)\n",
    "    X = scaler.transform(df[features].values)\n",
    "\n",
    "    seqs = create_sequences(X, L)\n",
    "    if seqs.size == 0:\n",
    "        return np.array([]), np.array([]), 0, None \n",
    "\n",
    "    tens = torch.tensor(seqs)\n",
    "    dl = DataLoader(TensorDataset(tens, tens), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    scores = predict_scores(model, dl, mode)\n",
    "\n",
    "    seq_labels = np.array([], dtype=int)\n",
    "    seq_atk_start_seq = 0   \n",
    "    atk_row_start = None   \n",
    "\n",
    "    if \"label\" in df.columns and len(df) >= L:\n",
    "        row_labels = (\n",
    "            (df[\"label\"].astype(str).str.upper().isin([\"1\",\"T\",\"TRUE\"])) |\n",
    "            (df[\"label\"].astype(str).isin([\"1\"]))\n",
    "        ).astype(int).to_numpy()\n",
    "\n",
    "        seq_labels = np.array([1 if np.any(row_labels[i:i+L]) else 0\n",
    "                               for i in range(len(row_labels) - L + 1)], dtype=int)\n",
    "\n",
    "        idx_rows = np.where(row_labels == 1)[0]\n",
    "        if idx_rows.size:\n",
    "            r0 = int(idx_rows[0])\n",
    "            atk_row_start = r0\n",
    "            seq_atk_start_seq = max(0, r0 - (L - 1))\n",
    "\n",
    "    return scores, seq_labels, seq_atk_start_seq, atk_row_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTIVE alpha\n",
    "def order_stat_from_alpha(sorted_scores: np.ndarray, alpha_t: float):\n",
    "    n = sorted_scores.size\n",
    "    if n == 0:\n",
    "        return np.nan, 0\n",
    "    k = int(np.ceil((n + 1) * (1.0 - alpha_t)))\n",
    "    k = min(max(k, 1), n)\n",
    "    return float(sorted_scores[k - 1]), k\n",
    "\n",
    "def adaptive_stream(scores: np.ndarray,\n",
    "                    sorted_calib: np.ndarray,\n",
    "                    alpha0: float,\n",
    "                    gamma: float,\n",
    "                    alpha_minmax=(1e-6, 0.5)):\n",
    "    m = scores.size\n",
    "    alpha_t = np.empty(m, dtype=float)\n",
    "    q_t     = np.empty(m, dtype=float)\n",
    "    err_t   = np.empty(m, dtype=int)\n",
    "    y_hat   = np.empty(m, dtype=int)\n",
    "    cov     = np.empty(m, dtype=float)\n",
    "\n",
    "    a = alpha0\n",
    "    hits = 0\n",
    "    for t in range(m):\n",
    "        qt, _ = order_stat_from_alpha(sorted_calib, a)\n",
    "        q_t[t] = qt\n",
    "\n",
    "        y_hat[t] = int(scores[t] > qt)\n",
    "        err_t[t] = y_hat[t]\n",
    "\n",
    "        a = a + gamma * (alpha0 - err_t[t])\n",
    "        a = float(np.clip(a, alpha_minmax[0], alpha_minmax[1]))\n",
    "        alpha_t[t] = a\n",
    "\n",
    "        hits += (1 - err_t[t])\n",
    "        cov[t] = hits / (t + 1)\n",
    "\n",
    "    return {\n",
    "        \"y_hat\": y_hat,\n",
    "        \"alpha_t\": alpha_t,\n",
    "        \"q_t\": q_t,\n",
    "        \"err_t\": err_t,\n",
    "        \"coverage_t\": cov,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION: TRADITIONAL THRESHOLDING (NO CONFORMAL)\n",
    "def evaluate_traditional_thresholding(mode: str):\n",
    "    mode_dir = ARTIFACTS / f\"{mode}_traditional\"\n",
    "    mode_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    benign_scores = benign_scores_for_threshold(mode)\n",
    "    tau = compute_tau(benign_scores, TRAD_PCT)\n",
    "\n",
    "    plot_hist(\n",
    "        benign_scores,\n",
    "        title=f\"Benign Scores (Traditional – {mode})\",\n",
    "        xlabel=f\"{mode} score\",\n",
    "        save_path=mode_dir / \"benign_scores_traditional.png\",\n",
    "        vline=tau,\n",
    "        vlabel=rf\"$\\tau_{{{int(TRAD_PCT*100)}}} = {tau:.4f}$\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for name, path in DATASETS:\n",
    "        p = Path(path)\n",
    "        if not p.exists():\n",
    "            print(f\"[WARN] Missing: {path}\")\n",
    "            continue\n",
    "\n",
    "        scores, seq_labels, seq_atk_start_seq, atk_row_start = load_attack_scores(path, features, scaler, SEQUENCE_LENGTH, mode)\n",
    "        if scores.size == 0:\n",
    "            continue\n",
    "\n",
    "        yhat = (scores > tau).astype(int)\n",
    "\n",
    "        row = {\"type\": \"traditional\", \"mode\": mode, \"dataset\": name, \"num_seqs\": int(scores.size), \"threshold\": float(tau)}\n",
    "        if seq_labels.size:\n",
    "            acc = accuracy_score(seq_labels, yhat)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(seq_labels, yhat, average=\"binary\", zero_division=0)\n",
    "            cm = confusion_matrix(seq_labels, yhat, labels=[0,1])\n",
    "            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (np.nan,)*4\n",
    "            row.update(dict(acc=acc, prec=prec, rec=rec, f1=f1, tn=int(tn), fp=int(fp), fn=int(fn), tp=int(tp)))\n",
    "\n",
    "            plot_rows_timeline_traditional(\n",
    "                scores=scores,\n",
    "                L=SEQUENCE_LENGTH,\n",
    "                tau=tau,\n",
    "                atk_row_start=atk_row_start,\n",
    "                save_path=mode_dir / f\"{name}_rows_timeline_traditional.png\",\n",
    "                mode_label=mode,\n",
    "                attack_name=name,\n",
    "                smooth=25,\n",
    "                n_rows_view=N_ROWS_VIEW,\n",
    "                y_clip=0.995,\n",
    "            )\n",
    "\n",
    "            plot_cm(seq_labels, yhat,\n",
    "                    title=f\"{name.upper()} – Confusion Matrix (Traditional, {mode})\",\n",
    "                    save_path=mode_dir / f\"{name}_cm_traditional.png\")\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(mode_dir / \"summary_traditional.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "# EVALUATION: STANDARD CONFORMAL (FIXED quantile)\n",
    "def evaluate_standard_conformal(mode: str):\n",
    "    mode_dir = ARTIFACTS / f\"{mode}_standard\"\n",
    "    mode_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    calib_scores = predict_scores(model, calib_loader, mode)\n",
    "    q_hat = np.quantile(calib_scores, 1 - ALPHA)\n",
    "\n",
    "    plot_hist(\n",
    "        calib_scores,\n",
    "        title=f\"Calibration Scores (Standard Conformal – {mode})\",\n",
    "        xlabel=f\"{mode} score\",\n",
    "        save_path=mode_dir / \"calibration_scores_fixed.png\",\n",
    "        vline=q_hat,\n",
    "        vlabel=fr\"Fixed $\\hat{{q}}$ (α={ALPHA}) = {q_hat:.4f}\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for name, path in DATASETS:\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            print(f\"[WARN] Missing: {path}\")\n",
    "            continue\n",
    "\n",
    "        scores, seq_labels, seq_atk_start_seq, atk_row_start = load_attack_scores(path, features, scaler, SEQUENCE_LENGTH, mode)\n",
    "\n",
    "        if scores.size == 0:\n",
    "            continue\n",
    "\n",
    "        yhat = (scores > q_hat).astype(int)\n",
    "\n",
    "        row = {\"type\":\"standard\",\"mode\":mode,\"dataset\":name,\"num_seqs\":int(scores.size),\"threshold\":float(q_hat)}\n",
    "        if seq_labels.size:\n",
    "            acc = accuracy_score(seq_labels, yhat)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(seq_labels, yhat, average=\"binary\", zero_division=0)\n",
    "            cm = confusion_matrix(seq_labels, yhat, labels=[0,1])\n",
    "            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (np.nan,)*4\n",
    "            row.update(dict(acc=acc, prec=prec, rec=rec, f1=f1, tn=int(tn), fp=int(fp), fn=int(fn), tp=int(tp)))\n",
    "\n",
    "            plot_rows_timeline_fixed(\n",
    "                scores=scores,\n",
    "                seq_labels=seq_labels,\n",
    "                q_fixed=q_hat,\n",
    "                alpha=ALPHA,\n",
    "                L=SEQUENCE_LENGTH,\n",
    "                atk_row_start=atk_row_start,\n",
    "                save_path=mode_dir / f\"{name}_rows_timeline_fixed.png\",\n",
    "                mode_label=mode,\n",
    "                attack_name=name,\n",
    "                smooth=25,\n",
    "                n_rows_view=N_ROWS_VIEW,\n",
    "                y_clip=0.995,\n",
    "            )\n",
    "\n",
    "\n",
    "            plot_cm(seq_labels, yhat,\n",
    "                    title=f\"{name.upper()} – Confusion Matrix (Standard Conformal, {mode})\",\n",
    "                    save_path=mode_dir / f\"{name}_cm_fixed.png\")\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(mode_dir / \"summary_standard.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "# EVALUATION: ADAPTIVE CONFORMAL (VARYING alpha and QUANTILE)\n",
    "def evaluate_adaptive_conformal(mode: str):\n",
    "    mode_dir = ARTIFACTS / f\"{mode}_adaptive\"\n",
    "    mode_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    calib_scores = predict_scores(model, calib_loader, mode)\n",
    "    if calib_scores.size == 0:\n",
    "        print(\"[adaptive] No calibration scores.\")\n",
    "        return pd.DataFrame()\n",
    "    sorted_calib = np.sort(calib_scores)\n",
    "\n",
    "    plot_hist(\n",
    "        calib_scores,\n",
    "        title=f\"Calibration Scores (Adaptive Conformal – {mode})\",\n",
    "        xlabel=f\"{mode} score\",\n",
    "        save_path=mode_dir / \"calibration_scores_adaptive.png\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for name, path in DATASETS:\n",
    "        path_obj = Path(path)\n",
    "        if not path_obj.exists():\n",
    "            print(f\"[WARN] Missing: {path}\")\n",
    "            continue\n",
    "\n",
    "        scores, seq_labels, seq_atk_start_seq, atk_row_start = load_attack_scores(\n",
    "            path, features, scaler, SEQUENCE_LENGTH, mode\n",
    "        )\n",
    "        if scores.size == 0:\n",
    "            continue\n",
    "\n",
    "        out = adaptive_stream(\n",
    "            scores=scores,\n",
    "            sorted_calib=sorted_calib,\n",
    "            alpha0=ALPHA,\n",
    "            gamma=ADAPT_GAMMA,\n",
    "            alpha_minmax=ALPHA_MINMAX,\n",
    "        )\n",
    "        yhat = out[\"y_hat\"]\n",
    "\n",
    "        row = {\"type\": \"adaptive\", \"mode\": mode, \"dataset\": name, \"num_seqs\": int(scores.size)}\n",
    "        if seq_labels.size:\n",
    "            acc = accuracy_score(seq_labels, yhat)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "                seq_labels, yhat, average=\"binary\", zero_division=0\n",
    "            )\n",
    "            cm = confusion_matrix(seq_labels, yhat, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (np.nan,) * 4\n",
    "            row.update(dict(acc=acc, prec=prec, rec=rec, f1=f1, tn=int(tn), fp=int(fp), fn=int(fn), tp=int(tp)))\n",
    "\n",
    "            plot_rows_timeline_adaptive(\n",
    "                scores=scores,\n",
    "                seq_labels=seq_labels,\n",
    "                alpha_series=out[\"alpha_t\"],\n",
    "                q_series=out[\"q_t\"],\n",
    "                alpha0=ALPHA,\n",
    "                L=SEQUENCE_LENGTH,\n",
    "                atk_row_start=atk_row_start,\n",
    "                save_path=mode_dir / f\"{name}_rows_timeline_adaptive.png\",\n",
    "                mode_label=mode,\n",
    "                attack_name=name,\n",
    "                smooth=25,\n",
    "                n_rows_view=N_ROWS_VIEW,\n",
    "                y_clip=0.995,\n",
    "            )\n",
    "\n",
    "            plot_cm(seq_labels, yhat,\n",
    "                    title=f\"{name.upper()} – Confusion Matrix (Adaptive Conformal, {mode})\",\n",
    "                    save_path=mode_dir / f\"{name}_cm_adaptive.png\")\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(mode_dir / \"summary_adaptive.csv\", index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n===== Traditional Thresholding ({SELECT_SCORE}) =====\")\n",
    "df_trad  = evaluate_traditional_thresholding(SELECT_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n===== Standard Conformal ({SELECT_SCORE}) =====\")\n",
    "df_std   = evaluate_standard_conformal(SELECT_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n===== Adaptive Conformal ({SELECT_SCORE}) =====\")\n",
    "df_adapt = evaluate_adaptive_conformal(SELECT_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = [x for x in [df_trad, df_std, df_adapt] if x is not None and not x.empty]\n",
    "final_df = pd.concat(all_df, ignore_index=True) if all_df else pd.DataFrame()\n",
    "\n",
    "cols = [\"type\",\"mode\",\"dataset\",\"num_seqs\",\"threshold\",\"q_fixed\",\"acc\",\"prec\",\"rec\",\"f1\",\"tn\",\"fp\",\"fn\",\"tp\"]\n",
    "final_df = final_df.reindex(columns=[c for c in cols if c in final_df.columns])\n",
    "\n",
    "display(final_df)\n",
    "final_df.to_csv(ARTIFACTS / f\"final_metrics_{SELECT_SCORE.lower()}.csv\", index=False)\n",
    "print(f\"[saved] {ARTIFACTS / f'final_metrics_{SELECT_SCORE.lower()}.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8266798,
     "sourceId": 13054723,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 450850,
     "modelInstanceId": 433998,
     "sourceId": 581410,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
